{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61a9336-cf02-4a2d-bb35-819376cac80e",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "For the Training, I have created a container that effectively contains the code from notebooks I've found online:\n",
    "\n",
    "* YourTTS: \n",
    "* VITS for one speaker: https://colab.research.google.com/drive/1wAuG-TcZeAUYhff0f6ZiG-so9KT-sBIE?usp=sharing#scrollTo=psssJnAtRp4-\n",
    "* For VITS multispeaker I merged two aforementioned notebooks myself.\n",
    "\n",
    "I have also added some background operations:\n",
    "* CoquiTTS saves checkpoints (last 4 states of the models) and \"best\" models according to its metrics. So sometimes there are hours of training that do not produce best models and the checkpoints are deleted. So I added stashing of these intermediate states.\n",
    "* Inference from best- and stahed models, so we could hear how these models sound.\n",
    "* Drawing a plot from the training logs.\n",
    "\n",
    "## Why Docker container?\n",
    "\n",
    "Comparing with collab notebooks, containers have the following advantages:\n",
    "\n",
    "* Notebooks require step-by-step execution, which is not what I want: I want a process I can start from Python or command line and forget about it until it's done.\n",
    "* You can run notebooks at any machine, you only need SSH and Docker for this. Collab notebook you can only run, well, on collab. I believe Docker solution helps avoiding vendor lock.\n",
    "* It's easier to spread: if I need to deliver docker to a remote machine, I just need docker push and pull operations, no interation with server's GUI is required.\n",
    "* I don't trust very much the code written in notebooks: for instance, it's impossible to test it properly. Some of the background operations I've written have unit tests which wouldn't be possible with notebooks.\n",
    "* In general, I recommend using notebooks for what they were designed for - for experiments. I don't want experiments in this area, I want to a ready code that works.\n",
    "\n",
    "At the local machine, there was also an option to run this program under WSL (running it without VM wasn't an option because of espeak problems under Windows). **Avoid this option**. Despite WSL and docker actually use the same Windows subsystem, WSL works far worse and far less predictable. There were numerous problems with it:\n",
    "* First time I run it, it failed after 5 hours.\n",
    "* I \"fixed\" it with `kaia.infra.rebooter` that monitors the log file and restarts the system because of the inactivity. Effectively, it restarted every 10 minutes. This has proven to be bad for the training process itself: despite I continued training with CoquiTTS means, I had the impression it's kinda reset.\n",
    "* After several days of experiments, I've found out that actually running training _in a different console than the rebooter's console_ solves the problem. How exactly sharing console prevents WSL from working properly, I don't know.\n",
    "* After packaging the solution in Docker container, I've never had any problems with this process again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b934143-7829-4891-9934-6921711359e0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Training container is organized by the same architecture, as the docker-based deciders in BrainBox. There are `CoquiTrainingContainerSettings` and `CoquiTrainingContainerInstaller`. \n",
    "\n",
    "First, we need to create container. This will take awhile. Check for progress in the console from which Jupyter Lab was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db5d8b9-ee96-4061-9be8-315c0380ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.ml.voice_cloning.coqui_training_container import CoquiTrainingContainerSettings, CoquiTrainingContainerInstaller, SupportedModels\n",
    "\n",
    "settings = CoquiTrainingContainerSettings()\n",
    "installer = CoquiTrainingContainerInstaller(settings)\n",
    "installer.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b7b8c-5eb1-49a3-aa60-e73aebac9c08",
   "metadata": {},
   "source": [
    "Then, we need to place a model inside the working folder. The models are downloaded for `CoquiTTS` decider, so this is where we can take them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fa2e2d-0143-4f81-887b-6366b32fa62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tts_models--en--vctk--vits',\n",
       " 'tts_models--multilingual--multi-dataset--xtts_v2',\n",
       " 'tts_models--multilingual--multi-dataset--your_tts']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from kaia.brainbox.deciders.docker_based import CoquiTTSSettings\n",
    "\n",
    "coqui_tts_settings = CoquiTTSSettings()\n",
    "source_folder = coqui_tts_settings.resource_folder/'builtin'\n",
    "models = os.listdir(source_folder)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9301da-35bb-414d-acd2-9d3d6befe97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json', 'model_file.pth', 'speaker_ids.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "model = [m for m in models if 'vits' in m][0]\n",
    "dst_folder = settings.resource_folder/'model'\n",
    "shutil.rmtree(dst_folder, ignore_errors=True)\n",
    "shutil.copytree(source_folder/model, dst_folder)\n",
    "os.listdir(dst_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b8220-6cfb-49fb-b664-2ffc795ad5d1",
   "metadata": {},
   "source": [
    "Now, we also need a media library to train on. We can't create and annotate a library with TortoiseTTS, it's too laborious. Instead, we will generate samples with couple of the voices from VITS model, and then train VITS model on them. This is in general meaningless, because we basically train the model on its own output, but it will help us to see that container works normally.\n",
    "\n",
    "Run BrainBox and then execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e9ab54-99bd-407f-afbc-cb6505a143b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.ml.voice_cloning.data_prep.task_generator import generate_tasks\n",
    "from kaia.brainbox import BrainBoxTask, BrainBox\n",
    "from kaia.infra import FileIO\n",
    "\n",
    "def create_coqui_task(voice, text):\n",
    "    return BrainBoxTask(\n",
    "        id=BrainBoxTask.safe_id(),\n",
    "        decider='CoquiTTS',\n",
    "        decider_parameters='tts_models/en/vctk/vits',\n",
    "        decider_method='dub',\n",
    "        arguments=dict(voice=voice, text=text),\n",
    "    )\n",
    "\n",
    "sentences = [item['text'] for item in FileIO.read_json('files/golden_set.json')]\n",
    "voices = ['p225','p229']\n",
    "\n",
    "tasks = generate_tasks(sentences,voices,None,create_coqui_task, dict(selected=True))\n",
    "api = BrainBox().create_api('127.0.0.1')\n",
    "#result = api.execute(tasks); api.download(result,settings.resource_folder/'test_dataset.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286c7580-d751-4002-9f59-3960fc0df918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset.zip', 'model', 'test_dataset.zip', 'training']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(settings.resource_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a5e86-321c-4b95-9942-83976088262c",
   "metadata": {},
   "source": [
    "Now, we are ready to start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8699a35e-b0ae-4a0a-ac29-2fde6d6c5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installer.create_train_process(SupportedModels.VitsMultispeaker, 'model/model_file.pth', 'test_dataset.zip').run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042ca8-d8b0-4306-bd2d-6658abbf8010",
   "metadata": {},
   "source": [
    "The following folder are located inside `settings.resource_folder/'training'`:\n",
    "\n",
    "* Created by data transformation:\n",
    "  * `data` contains the unziped and transformed dataset in the format, required by VITS/Yourtts (they are different in the recipies I used, although I think they can be unified)\n",
    "  * `temp` is not important\n",
    "* Created by CoquiTTS itself\n",
    "  * `training_output` contains everything important from the training, including\n",
    "    * Training log\n",
    "    * Best models: the model's states that are selected by CoquiTTS\n",
    "    * Checkpoints: last states of the model that are required to restart\n",
    "    * Stashed models: my addon. Sometimes there are huge gaps between the best states, and I wanted to evaluate the model in-between as well. So I wrote a threaded service that stash checkpoints.\n",
    "  * `model` contains a json config, the Coqui process creates it, it's not important\n",
    "* Created by sampler:\n",
    "  * I thought it's good to have an opportunity to listen to the models, so `samples` contains the voiceover of one sentence by all the best and stashed models. You can evaluate the models \"by ear\" and decide when to stop the training.\n",
    " \n",
    "You may use the script from `kaia.ml.voice_cloning.data_prep.quality_assurance` to concatenate sounds into one video with indices. This script however doesn't always work for some obscure FFMPEG-related reasons.\n",
    "\n",
    "Most importantly, you may export the resulting model to `CoquiTTS` decider and use it for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26d5256-16fc-4094-8fb4-43e1796a18fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'custom_models/test.pth'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaia.ml.voice_cloning.coqui_training_container.container.utils import get_working_folder\n",
    "from kaia.ml.voice_cloning.coqui_training_container.container.model_file_info import ModelFileInfo\n",
    "\n",
    "training_output = settings.resource_folder/'training/training_output/'\n",
    "working_folder = get_working_folder(training_output)\n",
    "models = ModelFileInfo.parse_folder(working_folder)\n",
    "best_model_path = [m for m in models if m.type==ModelFileInfo.Type.BestModel][0].path\n",
    "\n",
    "model_name = coqui_tts_settings.export_model_from_training(best_model_path, 'test')\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b534945-91fc-4df5-9e5d-ef83a7e1d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.brainbox.deciders.docker_based import CoquiTTSInstaller\n",
    "\n",
    "installer = CoquiTTSInstaller(coqui_tts_settings)\n",
    "coqui_tts_api = installer.run_in_any_case_and_create_api()\n",
    "model_info = coqui_tts_api.load_model(model_name)\n",
    "voiceover = coqui_tts_api.dub(model_info, 'Hello, world!', model_info['speakers'][0])\n",
    "with open('files/hello_world.wav','wb') as file:\n",
    "    file.write(voiceover)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a70f48-29c3-4d1c-bc82-fc0c9a30a65c",
   "metadata": {},
   "source": [
    "I have saved the resulting audio so you could enjoy it without running the whole thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c988fd4-7b85-4d85-a755-1c0d8e111cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32b808faff045ef9948723c7bcf9004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Audio(value=b'RIFFD4\\x01\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\"V\\x00\\x00D\\xac\\x00\\x00\\x02\\x00\\x10\\x00datâ€¦"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Audio\n",
    "with open('files/hello_world.wav','rb') as file:\n",
    "    voiceover = file.read()\n",
    "Audio(value=voiceover, autoplay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d3cef-be9b-4294-91bd-4a2355aedd6b",
   "metadata": {},
   "source": [
    "The quality is not the best. However, it will greatly improved if using more samples and if the samples will come from TortoiseTTS and not from CoquiTTS, as in the sample. What is clear, however, that the training container works and you may now create fast voiceovers with an okeyish quality for your favourite characters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
