{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa408c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca43cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1128\n",
      "Unique lines: 1098\n"
     ]
    }
   ],
   "source": [
    "input_file = \"chat_draft_list_data.txt\"\n",
    "output_file = \"list_data.jsonl\"\n",
    "\n",
    "unique_lines = set()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    for line in all_lines:\n",
    "        unique_lines.add(line.strip())\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in unique_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Total lines: {len(all_lines)}\")\n",
    "print(f\"Unique lines: {len(unique_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"list_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "random.shuffle(lines)\n",
    "\n",
    "val_len = int(0.2 * len(lines))\n",
    "\n",
    "val_lines = lines[:val_len]\n",
    "train_lines = lines[val_len:]\n",
    "\n",
    "with open(\"val_list.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_lines)\n",
    "\n",
    "with open(\"train_list.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d738a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", data_files={\"train\": \"train_list.jsonl\", \"validation\": \"val_list.jsonl\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d1beaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'For the party snacks: nuts, dried fruit, pretzels, cheese dip',\n",
       " 'LIST': ['nuts', 'dried fruit', 'pretzels', 'cheese dip']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37d7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    user = sample[\"USER\"]\n",
    "    items = sample[\"LIST\"]\n",
    "    items_str = \",\".join(items)\n",
    "    return f\"USER:{user}\\nLIST:{items_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56157e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].map(lambda x: {\"text\": format_prompt(x)})\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(lambda x: {\"text\": format_prompt(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9882bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'For the party snacks: nuts, dried fruit, pretzels, cheese dip',\n",
       " 'LIST': ['nuts', 'dried fruit', 'pretzels', 'cheese dip'],\n",
       " 'text': 'USER:For the party snacks: nuts, dried fruit, pretzels, cheese dip\\nLIST:nuts,dried fruit,pretzels,cheese dip'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f03c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:For the party snacks: nuts, dried fruit, pretzels, cheese dip\n",
      "LIST:nuts,dried fruit,pretzels,cheese dip\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a25cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google/gemma-3-270m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfe932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75553b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a99a72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3c930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8aef736a64e40b1a2b14e4e76668152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fad66abd154f418e791d0039f8458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    prompt = f\"USER:{sample[\"USER\"]}\\n\"\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    tokenized = tokenizer(sample[\"text\"], padding=\"max_length\", max_length=41)\n",
    "    pad_len = tokenized[\"input_ids\"].count(tokenizer.pad_token_id)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    tokenized[\"labels\"][: pad_len + prompt_len] = [-100] * (pad_len + prompt_len)\n",
    "    return tokenized\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(tokenize, batched=False)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65982426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['USER', 'LIST', 'text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93215b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 20791, 236787, 2542, 506, 4598, 35009, 236787, 22350, 236764, 19555, 9479, 236764, 205807, 236764, 13840, 12372, 107, 22919, 236787, 36766, 236764, 89754, 9479, 236764, 1734, 21448, 1416, 236764, 120563, 12372]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 22919, 236787, 36766, 236764, 89754, 9479, 236764, 1734, 21448, 1416, 236764, 120563, 12372]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"attention_mask\"])\n",
    "print(dataset[\"train\"][0][\"input_ids\"])\n",
    "print(dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb50704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f130e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204c40fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bc4e59ade445c78fc2f0f9c51f721f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bf088beb68454d9fe8aefa2bbf5c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmp1yyg4_s4/test.c -o /tmp/tmp1yyg4_s4/test.o\n",
      "INFO:root:cc -pthread /tmp/tmp1yyg4_s4/test.o -laio -o /tmp/tmp1yyg4_s4/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmp1egwokwq/test.c -o /tmp/tmp1egwokwq/test.o\n",
      "INFO:root:cc -pthread /tmp/tmp1egwokwq/test.o -L/usr/local/cuda-12.6 -L/usr/local/cuda-12.6/lib64 -lcufile -o /tmp/tmp1egwokwq/a.out\n",
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmp8lhy8w_a/test.c -o /tmp/tmp8lhy8w_a/test.o\n",
      "INFO:root:cc -pthread /tmp/tmp8lhy8w_a/test.o -laio -o /tmp/tmp8lhy8w_a/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 01:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.068669</td>\n",
       "      <td>2.104072</td>\n",
       "      <td>64903.000000</td>\n",
       "      <td>0.981425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.032564</td>\n",
       "      <td>2.515906</td>\n",
       "      <td>129109.000000</td>\n",
       "      <td>0.994735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>1.996809</td>\n",
       "      <td>193315.000000</td>\n",
       "      <td>0.995645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.027633</td>\n",
       "      <td>2.302610</td>\n",
       "      <td>257521.000000</td>\n",
       "      <td>0.995613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>2.658816</td>\n",
       "      <td>322424.000000</td>\n",
       "      <td>0.996102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=0.16191645128918544, metrics={'train_runtime': 102.5945, 'train_samples_per_second': 85.677, 'train_steps_per_second': 1.365, 'total_flos': 218533207472640.0, 'train_loss': 0.16191645128918544, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-timer-lora-list\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64bbb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gemma-timer-lora-list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094dc3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(sample[\"input_ids\"]) for sample in dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f14505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(sample[\"input_ids\"]) for sample in dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18c15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840cd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f81ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./gemma-timer-lora-list\", device_map=\"cuda\")\n",
    "text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74656abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    dataset,\n",
    "    log_file,\n",
    "    batch_size=64,\n",
    "):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with open(log_file, \"w\") as file:\n",
    "        for i in tqdm(range(0, total, batch_size)):\n",
    "            texts = dataset[i : i + batch_size][\"text\"]\n",
    "            prefixes = [text.split(\"\\n\")[0] + \"\\n\" for text in texts]\n",
    "            gen_outs = text_gen(prefixes, max_new_tokens=max_new_tokens, num_beams=1, do_sample=False)\n",
    "            for text, gen_out in zip(texts, gen_outs):\n",
    "                gen_text = gen_out[0][\"generated_text\"]\n",
    "                if len(gen_text) >= len(text) and text == gen_text[: len(text)]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"Mismatch: {text} -> {gen_text}\\n\", file=file)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d52a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [05:20<00:00, 80.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_accuracy(dataset[\"validation\"], log_file=\"eval_list.log\")\n",
    "print(f\"Validation accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e6d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: USER:Pasta dinner: penne, Alfredo sauce, garlic bread\n",
      "LIST:penne,Alfredo sauce,garlic bread -> USER:Pasta dinner: penne, Alfredo sauce, garlic bread\n",
      "LIST:penne,alfredo sauce,garlic bread,list\n",
      "LIST:penne,alfredo sauce,garlic bread,list\n",
      "LIST:penne,alfredo sauce,garlic bread\n",
      "\n",
      "Mismatch: USER:Veg box — broccoli, cauliflower, green beans\n",
      "LIST:broccoli,cauliflower,green beans -> USER:Veg box — broccoli, cauliflower, green beans\n",
      "LIST:broccoli,cucumbers,green beans,green beans,cucumbers,onions,caps,list\n",
      "LISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLISTLIST\n",
      "\n",
      "Mismatch: USER:I need a new kettle\n",
      "LIST:kettle -> USER:I need a new kettle\n",
      "LIST:new kettle,new kettle,new kettle,new kettle,list\n",
      "LIST:new kettle,new kettle,new kettle,list\n",
      "LIST:list,list,list,list,list\n",
      "LIST\n",
      "\n",
      "Mismatch: USER:Hmm… maybe just a loaf of sourdough\n",
      "LIST:loaf of sourdough -> USER:Hmm… maybe just a loaf of sourdough\n",
      "LIST:sourdough loaf,sourdough bread,sourdough bread,sourdough bread,sourdough bread,sourdough bread,sourdough bread,sourdough bread,\n",
      "\n",
      "Mismatch: USER:Gotta grab coffee beans and filters\n",
      "LIST:coffee beans,coffee filters -> USER:Gotta grab coffee beans and filters\n",
      "LIST:coffee beans,filters,coffee beans,filters\n",
      "LIST:coffee beans,filters,coffee beans,filters\n",
      "LIST:coffee beans,filters,coffee beans,filters\n",
      "LIST:coffee beans,filters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cat eval_list.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "pln"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
