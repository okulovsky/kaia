{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa408c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "input_file = \"datalines/chat_timer_data_eng_extended.jsonl\"\n",
    "output_file = \"datalines/unique_timers.jsonl\"\n",
    "\n",
    "unique_lines = set()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    for line in all_lines:\n",
    "        unique_lines.add(line.strip())\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in unique_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Total lines: {len(all_lines)}\")\n",
    "print(f\"Unique lines: {len(unique_lines)}\")\n",
    "\n",
    "with open(\"datalines/unique_timers.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "random.shuffle(lines)\n",
    "\n",
    "val_len = int(0.2 * len(lines))\n",
    "\n",
    "val_lines = lines[:val_len]\n",
    "train_lines = lines[val_len:]\n",
    "\n",
    "with open(\"datalines/val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_lines)\n",
    "\n",
    "with open(\"datalines/train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d738a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", data_files={\"train\": \"datalines/train.jsonl\", \"validation\": \"datalines/val.jsonl\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cb3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google/gemma-3-270m-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37d7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    user = sample[\"USER\"]\n",
    "    hours = sample[\"HOURS\"]\n",
    "    minutes = sample[\"MINUTES\"]\n",
    "    seconds = sample[\"SECONDS\"]\n",
    "    return f\"USER:{user}\\nHOURS:{hours}\\nMINUTES:{minutes}\\nSECONDS:{seconds}\" + tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56157e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].map(lambda x: {\"text\": format_prompt(x)})\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(lambda x: {\"text\": format_prompt(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9882bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'set a timer for 8 hours 9 minutes',\n",
       " 'HOURS': 8,\n",
       " 'MINUTES': 9,\n",
       " 'SECONDS': 0,\n",
       " 'text': 'USER:set a timer for 8 hours 9 minutes\\nHOURS:8\\nMINUTES:9\\nSECONDS:0<eos>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f03c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:set a timer for 8 hours 9 minutes\n",
      "HOURS:8\n",
      "MINUTES:9\n",
      "SECONDS:0<eos>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfe932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75553b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a99a72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3c930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a377702c3a4e378be3589b7c8164ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    prompt = f\"USER:{sample[\"USER\"]}\\n\"\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    tokenized = tokenizer(sample[\"text\"], padding=\"max_length\", max_length=45)\n",
    "    pad_len = tokenized[\"input_ids\"].count(tokenizer.pad_token_id)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    tokenized[\"labels\"][: pad_len + prompt_len] = [-100] * (pad_len + prompt_len)\n",
    "    return tokenized\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(tokenize, batched=False)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65982426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['USER', 'HOURS', 'MINUTES', 'SECONDS', 'text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93215b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 20791, 236787, 1025, 496, 20342, 573, 236743, 236828, 3885, 236743, 236819, 4310, 107, 10858, 66481, 236787, 236828, 107, 16008, 80914, 236787, 236819, 107, 149542, 236787, 236771, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 10858, 66481, 236787, 236828, 107, 16008, 80914, 236787, 236819, 107, 149542, 236787, 236771, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"attention_mask\"])\n",
    "print(dataset[\"train\"][0][\"input_ids\"])\n",
    "print(dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb50704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f130e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204c40fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5c30d882c4432c990a6f7836b5e7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 02:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.048274</td>\n",
       "      <td>1.184469</td>\n",
       "      <td>36000.000000</td>\n",
       "      <td>0.990206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.018838</td>\n",
       "      <td>0.567598</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>0.994447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>0.432888</td>\n",
       "      <td>108000.000000</td>\n",
       "      <td>0.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.524155</td>\n",
       "      <td>143550.000000</td>\n",
       "      <td>0.998597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.527374</td>\n",
       "      <td>179550.000000</td>\n",
       "      <td>0.999201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.522301</td>\n",
       "      <td>215550.000000</td>\n",
       "      <td>0.999298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.499526</td>\n",
       "      <td>251100.000000</td>\n",
       "      <td>0.999399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.491925</td>\n",
       "      <td>287100.000000</td>\n",
       "      <td>0.999399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.492619</td>\n",
       "      <td>323100.000000</td>\n",
       "      <td>0.999399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.492621</td>\n",
       "      <td>359100.000000</td>\n",
       "      <td>0.999399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=252, training_loss=0.11090628202059417, metrics={'train_runtime': 149.3863, 'train_samples_per_second': 53.78, 'train_steps_per_second': 1.687, 'total_flos': 225861546493440.0, 'train_loss': 0.11090628202059417, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-timer-lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    seed=887,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64bbb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gemma-timer-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# sample_template = \"HOURS:{}\\nMINUTES:{}\\nSECONDS:{}\" + tokenizer.eos_token\n",
    "\n",
    "# max_new_tokens = max(\n",
    "#     [\n",
    "#         len(tokenizer.tokenize(sample_template.format(h, m, s)))\n",
    "#         for h, m, s in product(range(100), repeat=3)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840cd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f81ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./gemma-timer-lora\", device_map=\"cuda\")\n",
    "text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74656abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    dataset,\n",
    "    log_file,\n",
    "    batch_size=64,\n",
    "):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with open(log_file, \"w\") as file:\n",
    "        for i in tqdm(range(0, total, batch_size)):\n",
    "            texts = [\n",
    "                text.replace(tokenizer.eos_token, \"\")\n",
    "                for text in dataset[i : i + batch_size][\"text\"]\n",
    "            ]\n",
    "            prefixes = [text.split(\"\\n\")[0] + \"\\n\" for text in texts]\n",
    "            gen_outs = text_gen(\n",
    "                prefixes,\n",
    "                do_sample=False,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "            for text, gen_out in zip(texts, gen_outs):\n",
    "                gen_text = gen_out[0][\"generated_text\"]\n",
    "                if len(gen_text) >= len(text) and text == gen_text[: len(text)]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"Mismatch: {text} -> {gen_text}\\n\", file=file)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d52a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:09<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_accuracy(dataset[\"validation\"], log_file=\"eval.log\")\n",
    "print(f\"Validation accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01e6d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: USER:half a minute and 22 seconds, go\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:52 -> USER:half a minute and 22 seconds, go\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:22\n",
      "\n",
      "Mismatch: USER:set a timer for half a minute and 30 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:60 -> USER:set a timer for half a minute and 30 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:30\n",
      "\n",
      "Mismatch: USER:half a minute plus 5 seconds, go ahead\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:35 -> USER:half a minute plus 5 seconds, go ahead\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:5\n",
      "\n",
      "Mismatch: USER:half a minute and 20 seconds, fire\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:50 -> USER:half a minute and 20 seconds, fire\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:40\n",
      "\n",
      "Mismatch: USER:start half a minute plus 20 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:50 -> USER:start half a minute plus 20 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cat eval.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama-cpp-repo'...\n",
      "remote: Enumerating objects: 70177, done.\u001b[K\n",
      "remote: Counting objects: 100% (353/353), done.\u001b[K\n",
      "remote: Compressing objects: 100% (228/228), done.\u001b[K\n",
      "remote: Total 70177 (delta 234), reused 127 (delta 125), pack-reused 69824 (from 3)\u001b[K\n",
      "Receiving objects: 100% (70177/70177), 216.13 MiB | 3.18 MiB/s, done.\n",
      "Resolving deltas: 100% (50721/50721), done.\n",
      "/home/dmitrievan/kaia_exps/kaia_exps/llama-cpp-repo/gguf-py\n",
      "\u001b[2mUsing Python 3.13.2 environment at: /home/dmitrievan/kaia_exps/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m4 packages\u001b[0m \u001b[2min 861ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 569ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0mfile:///home/dmitrievan/kaia_exps/\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1 (from file:///home/dmitrievan/kaia_exps/kaia_exps/llama-cpp-repo/gguf-py)\u001b[0m\n",
      "/home/dmitrievan/kaia_exps/kaia_exps\n",
      "--2025-12-02 16:57:53--  https://raw.githubusercontent.com/ggml-org/llama.cpp/master/convert_lora_to_gguf.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20603 (20K) [text/plain]\n",
      "Saving to: ‘convert_lora_to_gguf.py’\n",
      "\n",
      "convert_lora_to_ggu 100%[===================>]  20.12K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2025-12-02 16:57:53 (3.38 MB/s) - ‘convert_lora_to_gguf.py’ saved [20603/20603]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git llama-cpp-repo\n",
    "%cd llama-cpp-repo/gguf-py\n",
    "!uv pip install .\n",
    "%cd ../..\n",
    "%rm -rf llama-cpp-repo\n",
    "!wget https://raw.githubusercontent.com/ggml-org/llama.cpp/master/convert_lora_to_gguf.py\n",
    "!wget https://raw.githubusercontent.com/ggml-org/llama.cpp/master/convert_hf_to_gguf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b69139b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:lora-to-gguf:Loading base model from Hugging Face: google/gemma-3-270m-it\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:lora-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:gemma-3-270m-lora.gguf: n_tensors = 252, total_size = 15.2M\n",
      "Writing: 100%|███████████████████████████| 15.2M/15.2M [00:00<00:00, 830Mbyte/s]\n",
      "INFO:lora-to-gguf:Model successfully exported to gemma-3-270m-lora.gguf\n"
     ]
    }
   ],
   "source": [
    "!uv run convert_lora_to_gguf.py ./gemma-timer-lora --outfile gemma-3-270m-lora.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaia env",
   "language": "python",
   "name": "kaia_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
