{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa408c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be14dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"datalines/timer_name_data_de.jsonl\"\n",
    "output_file = \"datalines/timer_name_data_de_fixed.jsonl\"\n",
    "\n",
    "with (\n",
    "    open(input_file, \"r\", encoding=\"utf-8\") as f_in,\n",
    "    open(output_file, \"w\", encoding=\"utf-8\") as f_out,\n",
    "):\n",
    "    for line in f_in:\n",
    "        data = json.loads(line)\n",
    "        for key in [\"HOURS\", \"MINUTES\", \"SECONDS\"]:\n",
    "            if data[key] is None:\n",
    "                data[key] = 0\n",
    "        f_out.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca43cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eng] Unique lines: 1298\n",
      "[eng] → train: 1039, val: 259\n",
      "[rus] Unique lines: 1164\n",
      "[rus] → train: 932, val: 232\n",
      "[de] Unique lines: 1212\n",
      "[de] → train: 970, val: 242\n",
      "\n",
      "Total train: 2941 | Total val: 733\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "languages = [\"eng\", \"rus\", \"de\"]\n",
    "base_dir = \"datalines\"\n",
    "\n",
    "train_lines = []\n",
    "val_lines = []\n",
    "\n",
    "for lang in languages:\n",
    "    input_file = os.path.join(base_dir, f\"timer_name_data_{lang}.jsonl\")\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        unique_lines = list(set(line.strip() for line in f if line.strip()))\n",
    "\n",
    "    print(f\"[{lang}] Unique lines: {len(unique_lines)}\")\n",
    "\n",
    "    random.shuffle(unique_lines)\n",
    "    val_len = int(0.2 * len(unique_lines))\n",
    "    val_part = unique_lines[:val_len]\n",
    "    train_part = unique_lines[val_len:]\n",
    "\n",
    "    print(f\"[{lang}] → train: {len(train_part)}, val: {len(val_part)}\")\n",
    "\n",
    "    train_lines.extend(train_part)\n",
    "    val_lines.extend(val_part)\n",
    "\n",
    "random.shuffle(train_lines)\n",
    "random.shuffle(val_lines)\n",
    "\n",
    "train_path = os.path.join(base_dir, \"train_timer.jsonl\")\n",
    "val_path = os.path.join(base_dir, \"val_timer.jsonl\")\n",
    "\n",
    "with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in train_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(val_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in val_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\nTotal train: {len(train_lines)} | Total val: {len(val_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d738a1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc240b3b4a1445e2b3aaeae3780fe1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99e2b3267b94e009e5aa167139d1fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", data_files={\"train\": \"datalines/train_timer.jsonl\", \"validation\": \"datalines/val_timer.jsonl\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cb3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google/gemma-3-270m-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37d7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    user = sample[\"USER\"]\n",
    "    hours = sample[\"HOURS\"]\n",
    "    assert hours is not None\n",
    "    minutes = sample[\"MINUTES\"]\n",
    "    assert minutes is not None\n",
    "    seconds = sample[\"SECONDS\"]\n",
    "    assert seconds is not None\n",
    "    name = sample[\"NAME\"] or \"_\"\n",
    "    return f\"USER:{user}\\nHOURS:{hours}\\nMINUTES:{minutes}\\nSECONDS:{seconds}\\nNAME:{name}\" + tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56157e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f75c026b4e4a8fb2f7c3e3566d78d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de35396ce814e379a403c2141d57ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].map(lambda x: {\"text\": format_prompt(x)})\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(lambda x: {\"text\": format_prompt(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f610ed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'put on a seven hour ten minute timer for roast',\n",
       " 'HOURS': 7,\n",
       " 'MINUTES': 10,\n",
       " 'SECONDS': 0,\n",
       " 'NAME': 'roast',\n",
       " 'text': 'USER:put on a seven hour ten minute timer for roast\\nHOURS:7\\nMINUTES:10\\nSECONDS:0\\nNAME:roast<eos>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9882bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'Stell bitte einen 73 Sekunden Timer für den kurzen Test',\n",
       " 'HOURS': 0,\n",
       " 'MINUTES': 0,\n",
       " 'SECONDS': 73,\n",
       " 'NAME': None,\n",
       " 'text': 'USER:Stell bitte einen 73 Sekunden Timer für den kurzen Test\\nHOURS:0\\nMINUTES:0\\nSECONDS:73\\nNAME:_<eos>'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f03c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:put on a seven hour ten minute timer for roast\n",
      "HOURS:7\n",
      "MINUTES:10\n",
      "SECONDS:0\n",
      "NAME:roast<eos>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bfe932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75553b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb3c930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb251ef76284180a951357ce89fd801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5fed570f894187bfd4fe44067fe9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    prompt = f\"USER:{sample[\"USER\"]}\\n\"\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    tokenized = tokenizer(sample[\"text\"], padding=\"max_length\", max_length=53)\n",
    "    pad_len = tokenized[\"input_ids\"].count(tokenizer.pad_token_id)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    tokenized[\"labels\"][: pad_len + prompt_len] = [-100] * (pad_len + prompt_len)\n",
    "    return tokenized\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(tokenize, batched=False)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb50704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f130e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "204c40fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dbbfb00e2b4afba08b4971f6df51f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ba6c85bc9f4df59ed05f5c5368870e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmpth7iac4h/test.c -o /tmp/tmpth7iac4h/test.o\n",
      "INFO:root:cc -pthread /tmp/tmpth7iac4h/test.o -laio -o /tmp/tmpth7iac4h/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmpdfdqac4b/test.c -o /tmp/tmpdfdqac4b/test.o\n",
      "INFO:root:cc -pthread /tmp/tmpdfdqac4b/test.o -L/usr/local/cuda-12.6 -L/usr/local/cuda-12.6/lib64 -lcufile -o /tmp/tmpdfdqac4b/a.out\n",
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmprm7vh51j/test.c -o /tmp/tmprm7vh51j/test.o\n",
      "INFO:root:cc -pthread /tmp/tmprm7vh51j/test.o -laio -o /tmp/tmprm7vh51j/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 04:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.148852</td>\n",
       "      <td>0.964291</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>0.958938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.058358</td>\n",
       "      <td>0.847636</td>\n",
       "      <td>84800.000000</td>\n",
       "      <td>0.985805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.047155</td>\n",
       "      <td>0.563039</td>\n",
       "      <td>127200.000000</td>\n",
       "      <td>0.989627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.042242</td>\n",
       "      <td>0.648839</td>\n",
       "      <td>169441.000000</td>\n",
       "      <td>0.991557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.036993</td>\n",
       "      <td>0.742724</td>\n",
       "      <td>211841.000000</td>\n",
       "      <td>0.990569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.029238</td>\n",
       "      <td>0.903380</td>\n",
       "      <td>254241.000000</td>\n",
       "      <td>0.993263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>0.880565</td>\n",
       "      <td>296641.000000</td>\n",
       "      <td>0.993982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.935706</td>\n",
       "      <td>338882.000000</td>\n",
       "      <td>0.994202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>0.799334</td>\n",
       "      <td>381282.000000</td>\n",
       "      <td>0.994556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.023126</td>\n",
       "      <td>0.719220</td>\n",
       "      <td>423682.000000</td>\n",
       "      <td>0.994845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.022651</td>\n",
       "      <td>0.669216</td>\n",
       "      <td>466082.000000</td>\n",
       "      <td>0.994845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.022713</td>\n",
       "      <td>0.678670</td>\n",
       "      <td>508323.000000</td>\n",
       "      <td>0.995131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.606644</td>\n",
       "      <td>550723.000000</td>\n",
       "      <td>0.994697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.023170</td>\n",
       "      <td>0.611683</td>\n",
       "      <td>593123.000000</td>\n",
       "      <td>0.994912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.022975</td>\n",
       "      <td>0.578941</td>\n",
       "      <td>635364.000000</td>\n",
       "      <td>0.994615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.582007</td>\n",
       "      <td>677764.000000</td>\n",
       "      <td>0.994982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.022864</td>\n",
       "      <td>0.578295</td>\n",
       "      <td>720164.000000</td>\n",
       "      <td>0.995051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.022896</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>762564.000000</td>\n",
       "      <td>0.994983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=460, training_loss=0.1131415413611609, metrics={'train_runtime': 293.9904, 'train_samples_per_second': 50.019, 'train_steps_per_second': 1.565, 'total_flos': 486898968779520.0, 'train_loss': 0.1131415413611609, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-timer-name-lora\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    seed=887,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64bbb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gemma-timer-name-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840cd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f81ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./gemma-timer-name-lora\", device_map=\"cuda\")\n",
    "text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74656abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    dataset,\n",
    "    log_file,\n",
    "    batch_size=64,\n",
    "):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with open(log_file, \"w\") as file:\n",
    "        for i in tqdm(range(0, total, batch_size)):\n",
    "            texts = [\n",
    "                text.replace(tokenizer.eos_token, \"\")\n",
    "                for text in dataset[i : i + batch_size][\"text\"]\n",
    "            ]\n",
    "            prefixes = [text.split(\"\\n\")[0] + \"\\n\" for text in texts]\n",
    "            gen_outs = text_gen(\n",
    "                prefixes,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "            for text, gen_out in zip(texts, gen_outs):\n",
    "                gen_text = gen_out[0][\"generated_text\"]\n",
    "                if len(gen_text) >= len(text) and text == gen_text[: len(text)]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"Mismatch: {text} -> {gen_text}\\n\", file=file)\n",
    "\n",
    "    print(f\"Validation accuracy: {correct / total:.4f}\")\n",
    "    print(f\"Correct: {correct}, total: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d52a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:14<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9168\n",
      "Correct: 672, total: 733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_accuracy(dataset[\"validation\"], log_file=\"eval.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01e6d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: USER:Kannst du einen 6-Minuten-Timer für das Brot starten?\n",
      "HOURS:0\n",
      "MINUTES:6\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Kannst du einen 6-Minuten-Timer für das Brot starten?\n",
      "HOURS:0\n",
      "MINUTES:6\n",
      "SECONDS:0\n",
      "NAME:Brot\n",
      "\n",
      "Mismatch: USER:Setz mir einen 66 Sekunden Timer, nur Probe\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:66\n",
      "NAME:_ -> USER:Setz mir einen 66 Sekunden Timer, nur Probe\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:66\n",
      "NAME:Probe\n",
      "\n",
      "Mismatch: USER:Starte bitte einen 81-Sekunden-Timer\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:21\n",
      "NAME:_ -> USER:Starte bitte einen 81-Sekunden-Timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:81\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:launch timer for studying in 90 minutes\n",
      "HOURS:1\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:studying -> USER:launch timer for studying in 90 minutes\n",
      "HOURS:0\n",
      "MINUTES:90\n",
      "SECONDS:0\n",
      "NAME:studying\n",
      "\n",
      "Mismatch: USER:start a timer to remind me in 33 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:33\n",
      "NAME:_ -> USER:start a timer to remind me in 33 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:33\n",
      "NAME:reminder\n",
      "\n",
      "Mismatch: USER:Bitte starte einen 52-Sekunden-Timer für die Intervalle\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:52\n",
      "NAME:Intervalle -> USER:Bitte starte einen 52-Sekunden-Timer für die Intervalle\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:52\n",
      "NAME:Intervale\n",
      "\n",
      "Mismatch: USER:set a 73 second timer\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:13\n",
      "NAME:_ -> USER:set a 73 second timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Könntest du einen 66-Sekunden-Timer starten?\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:6\n",
      "NAME:_ -> USER:Könntest du einen 66-Sekunden-Timer starten?\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:66\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Mach bitte einen einstündigen Timer als Pause.\n",
      "HOURS:1\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:Pause -> USER:Mach bitte einen einstündigen Timer als Pause.\n",
      "HOURS:1\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:Pause\n",
      "\n",
      "Mismatch: USER:Mach einen 73-Sekunden-Timer für die Aufwärmübung.\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:Aufwärmen -> USER:Mach einen 73-Sekunden-Timer für die Aufwärmübung.\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:Aufwärmübung\n",
      "\n",
      "Mismatch: USER:set timer to clean room in 25 minutes\n",
      "HOURS:0\n",
      "MINUTES:25\n",
      "SECONDS:0\n",
      "NAME:clean room -> USER:set timer to clean room in 25 minutes\n",
      "HOURS:0\n",
      "MINUTES:25\n",
      "SECONDS:0\n",
      "NAME:room\n",
      "\n",
      "Mismatch: USER:start a timer to remind me after 50 minutes\n",
      "HOURS:0\n",
      "MINUTES:50\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:start a timer to remind me after 50 minutes\n",
      "HOURS:0\n",
      "MINUTES:50\n",
      "SECONDS:0\n",
      "NAME:reminder\n",
      "\n",
      "Mismatch: USER:start timer to remind me in four hours\n",
      "HOURS:4\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:start timer to remind me in four hours\n",
      "HOURS:4\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:reminder\n",
      "\n",
      "Mismatch: USER:please start a 90 second timer for jumping jacks\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:90\n",
      "NAME:jumping jacks -> USER:please start a 90 second timer for jumping jacks\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:30\n",
      "NAME:jumping jacks\n",
      "\n",
      "Mismatch: USER:bitte stell einen 99-Minuten-Timer für mein Experiment\n",
      "HOURS:1\n",
      "MINUTES:39\n",
      "SECONDS:0\n",
      "NAME:Experiment -> USER:bitte stell einen 99-Minuten-Timer für mein Experiment\n",
      "HOURS:0\n",
      "MINUTES:99\n",
      "SECONDS:0\n",
      "NAME:Experiment\n",
      "\n",
      "Mismatch: USER:Leg einen 11-Stunden-Timer wegen der Pflanzenbewässerung\n",
      "HOURS:11\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:Pflanzenbewässerung -> USER:Leg einen 11-Stunden-Timer wegen der Pflanzenbewässerung\n",
      "HOURS:11\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:Pflanzen\n",
      "\n",
      "Mismatch: USER:установи таймер на 99 минут\n",
      "HOURS:1\n",
      "MINUTES:39\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:установи таймер на 99 минут\n",
      "HOURS:0\n",
      "MINUTES:99\n",
      "SECONDS:0\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:запусти таймер на тринадцать минут и пометь как разминка\n",
      "HOURS:0\n",
      "MINUTES:13\n",
      "SECONDS:0\n",
      "NAME:разминка -> USER:запусти таймер на тринадцать минут и пометь как разминка\n",
      "HOURS:0\n",
      "MINUTES:13\n",
      "SECONDS:0\n",
      "NAME:минера\n",
      "\n",
      "Mismatch: USER:set a three hour timer to marinate the meat\n",
      "HOURS:3\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:marinade -> USER:set a three hour timer to marinate the meat\n",
      "HOURS:3\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:meat\n",
      "\n",
      "Mismatch: USER:please start a seventy nine second timer\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:19\n",
      "NAME:_ -> USER:please start a seventy nine second timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:79\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:launch a 73 second timer now\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:_ -> USER:launch a 73 second timer now\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:launch\n",
      "\n",
      "Mismatch: USER:Mach mal einen 99-Minuten-Timer\n",
      "HOURS:1\n",
      "MINUTES:39\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Mach mal einen 99-Minuten-Timer\n",
      "HOURS:0\n",
      "MINUTES:99\n",
      "SECONDS:0\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:start a twenty eight minute timer to practice guitar\n",
      "HOURS:0\n",
      "MINUTES:28\n",
      "SECONDS:0\n",
      "NAME:guitar practice -> USER:start a twenty eight minute timer to practice guitar\n",
      "HOURS:0\n",
      "MINUTES:28\n",
      "SECONDS:0\n",
      "NAME:guitar\n",
      "\n",
      "Mismatch: USER:поставь таймер на один час ровно\n",
      "HOURS:1\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:поставь таймер на один час ровно\n",
      "HOURS:1\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:ровно\n",
      "\n",
      "Mismatch: USER:установи таймер на 38 минут, чтобы успеть до звонка\n",
      "HOURS:0\n",
      "MINUTES:38\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:установи таймер на 38 минут, чтобы успеть до звонка\n",
      "HOURS:0\n",
      "MINUTES:38\n",
      "SECONDS:0\n",
      "NAME:звонок\n",
      "\n",
      "Mismatch: USER:Ich möchte einen 12-Stunden-Timer für die Gartenarbeit\n",
      "HOURS:12\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Ich möchte einen 12-Stunden-Timer für die Gartenarbeit\n",
      "HOURS:12\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:Gartenarbeit\n",
      "\n",
      "Mismatch: USER:Stell bitte einen 75-Minuten-Timer für die Gartenarbeit\n",
      "HOURS:1\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:Gartenarbeit -> USER:Stell bitte einen 75-Minuten-Timer für die Gartenarbeit\n",
      "HOURS:0\n",
      "MINUTES:75\n",
      "SECONDS:0\n",
      "NAME:Gartenarbeit\n",
      "\n",
      "Mismatch: USER:Setze einen Timer von einer Viertelstunde, danke.\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Setze einen Timer von einer Viertelstunde, danke.\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:Viertelstunde\n",
      "\n",
      "Mismatch: USER:put a 33 second timer for a quick sprint\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:33\n",
      "NAME:sprint -> USER:put a 33 second timer for a quick sprint\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:33\n",
      "NAME:quick sprint\n",
      "\n",
      "Mismatch: USER:put on a 73 second timer\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:13\n",
      "NAME:_ -> USER:put on a 73 second timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Starte einen Timer zwei Minuten und zwanzig Sekunden als Pause\n",
      "HOURS:0\n",
      "MINUTES:2\n",
      "SECONDS:20\n",
      "NAME:Pause -> USER:Starte einen Timer zwei Minuten und zwanzig Sekunden als Pause\n",
      "HOURS:0\n",
      "MINUTES:2\n",
      "SECONDS:0\n",
      "NAME:Pause\n",
      "\n",
      "Mismatch: USER:set timer for 14 minutes to stretch legs\n",
      "HOURS:0\n",
      "MINUTES:14\n",
      "SECONDS:0\n",
      "NAME:legs -> USER:set timer for 14 minutes to stretch legs\n",
      "HOURS:0\n",
      "MINUTES:14\n",
      "SECONDS:0\n",
      "NAME:leg stretching\n",
      "\n",
      "Mismatch: USER:Stell bitte einen 71-Minuten-Timer zum Lernen von Vokabeln\n",
      "HOURS:0\n",
      "MINUTES:71\n",
      "SECONDS:0\n",
      "NAME:Vokabeln lernen -> USER:Stell bitte einen 71-Minuten-Timer zum Lernen von Vokabeln\n",
      "HOURS:0\n",
      "MINUTES:71\n",
      "SECONDS:0\n",
      "NAME:Vokabeln\n",
      "\n",
      "Mismatch: USER:Setz einen 90-Sekunden-Timer\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:30\n",
      "NAME:_ -> USER:Setz einen 90-Sekunden-Timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:90\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Mach einen 2-Stunden-und-30-Minuten-Timer für die Reiszubereitung.\n",
      "HOURS:2\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:Reis kochen -> USER:Mach einen 2-Stunden-und-30-Minuten-Timer für die Reiszubereitung.\n",
      "HOURS:2\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:Reiszubereitung\n",
      "\n",
      "Mismatch: USER:поставь таймер на 73 секунды\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:13\n",
      "NAME:_ -> USER:поставь таймер на 73 секунды\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Kannst du mir einen 2-Stunden-15-Minuten-Timer für das Abendessen setzen\n",
      "HOURS:2\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Kannst du mir einen 2-Stunden-15-Minuten-Timer für das Abendessen setzen\n",
      "HOURS:2\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:Abendessen\n",
      "\n",
      "Mismatch: USER:setz einen Timer auf eine Dreiviertelstunde\n",
      "HOURS:0\n",
      "MINUTES:45\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:setz einen Timer auf eine Dreiviertelstunde\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:please start a timer for twenty two seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:22\n",
      "NAME:_ -> USER:please start a timer for twenty two seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:22\n",
      "NAME:twenty two seconds\n",
      "\n",
      "Mismatch: USER:включи таймер на девяносто девять минут\n",
      "HOURS:1\n",
      "MINUTES:39\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:включи таймер на девяносто девять минут\n",
      "HOURS:0\n",
      "MINUTES:99\n",
      "SECONDS:0\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Stell mir einen 75-Minuten-Timer für den Kuchen\n",
      "HOURS:1\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:Kuchen -> USER:Stell mir einen 75-Minuten-Timer für den Kuchen\n",
      "HOURS:0\n",
      "MINUTES:75\n",
      "SECONDS:0\n",
      "NAME:Kuchen\n",
      "\n",
      "Mismatch: USER:please start a timer for seventy five seconds\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:15\n",
      "NAME:_ -> USER:please start a timer for seventy five seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:75\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Starte einen 45-Minuten-Timer für meine Yoga-Übungen\n",
      "HOURS:0\n",
      "MINUTES:45\n",
      "SECONDS:0\n",
      "NAME:Yoga-Übungen -> USER:Starte einen 45-Minuten-Timer für meine Yoga-Übungen\n",
      "HOURS:0\n",
      "MINUTES:45\n",
      "SECONDS:0\n",
      "NAME:Yoga\n",
      "\n",
      "Mismatch: USER:I need a 50 minute timer for my yoga session.\n",
      "HOURS:0\n",
      "MINUTES:50\n",
      "SECONDS:0\n",
      "NAME:yoga session -> USER:I need a 50 minute timer for my yoga session.\n",
      "HOURS:0\n",
      "MINUTES:50\n",
      "SECONDS:0\n",
      "NAME:yoga\n",
      "\n",
      "Mismatch: USER:Starte einen 72-Sekunden-Timer zum Aufwärmen\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:12\n",
      "NAME:Aufwärmen -> USER:Starte einen 72-Sekunden-Timer zum Aufwärmen\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:72\n",
      "NAME:Aufwärmen\n",
      "\n",
      "Mismatch: USER:поставь таймер на 60 секунд\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:поставь таймер на 60 секунд\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:60\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Kannst du bitte einen 12 Stunden Timer als Einnahme-Erinnerung setzen\n",
      "HOURS:12\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:Medikation -> USER:Kannst du bitte einen 12 Stunden Timer als Einnahme-Erinnerung setzen\n",
      "HOURS:12\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:Einnahme-Erinnerung\n",
      "\n",
      "Mismatch: USER:Starte einen 58 Minuten Timer beim Lernen für die Prüfung\n",
      "HOURS:0\n",
      "MINUTES:58\n",
      "SECONDS:0\n",
      "NAME:Prüfungsvorbereitung -> USER:Starte einen 58 Minuten Timer beim Lernen für die Prüfung\n",
      "HOURS:0\n",
      "MINUTES:58\n",
      "SECONDS:0\n",
      "NAME:Prüfung\n",
      "\n",
      "Mismatch: USER:Bitte mach einen 90-Minuten-Timer als Mittagspause\n",
      "HOURS:1\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:Mittagspause -> USER:Bitte mach einen 90-Minuten-Timer als Mittagspause\n",
      "HOURS:0\n",
      "MINUTES:90\n",
      "SECONDS:0\n",
      "NAME:Mittagspause\n",
      "\n",
      "Mismatch: USER:Mach mir einen 33-Minuten-Timer als Kaffee-Pause\n",
      "HOURS:0\n",
      "MINUTES:33\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Mach mir einen 33-Minuten-Timer als Kaffee-Pause\n",
      "HOURS:0\n",
      "MINUTES:33\n",
      "SECONDS:0\n",
      "NAME:Kaffee-Pause\n",
      "\n",
      "Mismatch: USER:поставь таймер на 2 минуты, чтобы проверить почту\n",
      "HOURS:0\n",
      "MINUTES:2\n",
      "SECONDS:0\n",
      "NAME:почта -> USER:поставь таймер на 2 минуты, чтобы проверить почту\n",
      "HOURS:0\n",
      "MINUTES:2\n",
      "SECONDS:0\n",
      "NAME:чтение почты\n",
      "\n",
      "Mismatch: USER:установи таймер на 68 минут для учебной сессии\n",
      "HOURS:0\n",
      "MINUTES:68\n",
      "SECONDS:0\n",
      "NAME:учёба -> USER:установи таймер на 68 минут для учебной сессии\n",
      "HOURS:0\n",
      "MINUTES:68\n",
      "SECONDS:0\n",
      "NAME:учебная сессия\n",
      "\n",
      "Mismatch: USER:включи таймер на 30 секунд чтобы взять паузу\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:30\n",
      "NAME:пауза -> USER:включи таймер на 30 секунд чтобы взять паузу\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:30\n",
      "NAME: пауза\n",
      "\n",
      "Mismatch: USER:start a timer for seventy five minutes\n",
      "HOURS:1\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:start a timer for seventy five minutes\n",
      "HOURS:0\n",
      "MINUTES:75\n",
      "SECONDS:0\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Stell einen Timer auf eine halbe Stunde, um das Brot zu ruhen\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Stell einen Timer auf eine halbe Stunde, um das Brot zu ruhen\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "NAME:Brot ruhen\n",
      "\n",
      "Mismatch: USER:Mach einen 9-Minuten-Timer für mein Workout\n",
      "HOURS:0\n",
      "MINUTES:9\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Mach einen 9-Minuten-Timer für mein Workout\n",
      "HOURS:0\n",
      "MINUTES:9\n",
      "SECONDS:0\n",
      "NAME:Workout\n",
      "\n",
      "Mismatch: USER:Starte einen achtundsiebzig-Minuten-Timer für die Vorlesung\n",
      "HOURS:0\n",
      "MINUTES:78\n",
      "SECONDS:0\n",
      "NAME:Vorlesung -> USER:Starte einen achtundsiebzig-Minuten-Timer für die Vorlesung\n",
      "HOURS:0\n",
      "MINUTES:28\n",
      "SECONDS:0\n",
      "NAME:Vorlesung\n",
      "\n",
      "Mismatch: USER:please create a timer of ninety nine seconds\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:39\n",
      "NAME:_ -> USER:please create a timer of ninety nine seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:99\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:set a timer of 73 seconds\n",
      "HOURS:0\n",
      "MINUTES:1\n",
      "SECONDS:13\n",
      "NAME:_ -> USER:set a timer of 73 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73\n",
      "NAME:_\n",
      "\n",
      "Mismatch: USER:Bitte mach einen 37 Minuten Timer, ich arbeite daran\n",
      "HOURS:0\n",
      "MINUTES:37\n",
      "SECONDS:0\n",
      "NAME:_ -> USER:Bitte mach einen 37 Minuten Timer, ich arbeite daran\n",
      "HOURS:0\n",
      "MINUTES:37\n",
      "SECONDS:0\n",
      "NAME:Arbeite\n",
      "\n",
      "Mismatch: USER:поставь таймер на пять часов для варенья\n",
      "HOURS:5\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:варенье -> USER:поставь таймер на пять часов для варенья\n",
      "HOURS:5\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "NAME:варенья\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cat eval.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a1bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:lora-to-gguf:Loading base model from Hugging Face: google/gemma-3-270m-it\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:lora-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_a,     torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_b,     torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_a,     torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_b,     torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_b,       torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_a,      torch.float32 --> F32, shape = {2048, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_b,      torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_a,      torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_b,      torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_b,        torch.float32 --> F32, shape = {16, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:gemma-timer-name-lora.gguf: n_tensors = 252, total_size = 15.2M\n",
      "Writing: 100%|███████████████████████████| 15.2M/15.2M [00:00<00:00, 769Mbyte/s]\n",
      "INFO:lora-to-gguf:Model successfully exported to gemma-timer-name-lora.gguf\n"
     ]
    }
   ],
   "source": [
    "!uv run convert_lora_to_gguf.py ./gemma-timer-name-lora --outfile gemma-timer-name-lora.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f621b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaia env",
   "language": "python",
   "name": "kaia_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
