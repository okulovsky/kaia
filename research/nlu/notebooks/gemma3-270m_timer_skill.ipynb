{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa408c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca43cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 3145\n",
      "Unique lines: 2902\n"
     ]
    }
   ],
   "source": [
    "input_file = \"chat_timer_data_eng.txt\"\n",
    "output_file = \"unique_timers.jsonl\"\n",
    "\n",
    "unique_lines = set()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    for line in all_lines:\n",
    "        unique_lines.add(line.strip())\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in unique_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Total lines: {len(all_lines)}\")\n",
    "print(f\"Unique lines: {len(unique_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"unique_timers.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "random.shuffle(lines)\n",
    "\n",
    "val_len = int(0.2 * len(lines))\n",
    "\n",
    "val_lines = lines[:val_len]\n",
    "train_lines = lines[val_len:]\n",
    "\n",
    "with open(\"val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_lines)\n",
    "\n",
    "with open(\"train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d738a1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ccb9000cfb432c8c1524bd61b0fa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce7516f81ed4cfeb1bb15010e6ae691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d1beaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'Start a timer for sixty six seconds',\n",
       " 'HOURS': 0,\n",
       " 'MINUTES': 0,\n",
       " 'SECONDS': 66}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37d7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    user = sample[\"USER\"]\n",
    "    hours = sample[\"HOURS\"]\n",
    "    minutes = sample[\"MINUTES\"]\n",
    "    seconds = sample[\"SECONDS\"]\n",
    "    return f\"USER:{user}\\nHOURS:{hours}\\nMINUTES:{minutes}\\nSECONDS:{seconds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56157e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a254c760124b1ea786aa7abe46a45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60172c86c2bf4069b72fa491f39d826a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].map(lambda x: {\"text\": format_prompt(x)})\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(lambda x: {\"text\": format_prompt(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9882bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USER': 'Start a timer for sixty six seconds',\n",
       " 'HOURS': 0,\n",
       " 'MINUTES': 0,\n",
       " 'SECONDS': 66,\n",
       " 'text': 'USER:Start a timer for sixty six seconds\\nHOURS:0\\nMINUTES:0\\nSECONDS:66'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f03c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:Start a timer for sixty six seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:66\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a25cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google/gemma-3-270m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfe932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75553b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer(sample[\"text\"])[\"input_ids\"]) for sample in dataset[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a99a72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3c930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e06a25a09a4d01917209273b76f8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e805258275a0402081d46dfc61f8b725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    prompt = f\"USER:{sample[\"USER\"]}\\n\"\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    tokenized = tokenizer(sample[\"text\"], padding=\"max_length\", max_length=42)\n",
    "    pad_len = tokenized[\"input_ids\"].count(tokenizer.pad_token_id)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    tokenized[\"labels\"][: pad_len + prompt_len] = [-100] * (pad_len + prompt_len)\n",
    "    return tokenized\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(tokenize, batched=False)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65982426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['USER', 'HOURS', 'MINUTES', 'SECONDS', 'text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93215b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 20791, 236787, 6302, 496, 20342, 573, 41607, 3962, 9093, 107, 10858, 66481, 236787, 236771, 107, 16008, 80914, 236787, 236771, 107, 149542, 236787, 236825, 236825]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 10858, 66481, 236787, 236771, 107, 16008, 80914, 236787, 236771, 107, 149542, 236787, 236825, 236825]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"attention_mask\"])\n",
    "print(dataset[\"train\"][0][\"input_ids\"])\n",
    "print(dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb50704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f130e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204c40fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8024ff440ce24fc790981a44fedecb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75dd4a003a34e54951d4364668f1550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmpvosx9183/test.c -o /tmp/tmpvosx9183/test.o\n",
      "INFO:root:cc -pthread /tmp/tmpvosx9183/test.o -laio -o /tmp/tmpvosx9183/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmpe_4euwzl/test.c -o /tmp/tmpe_4euwzl/test.o\n",
      "INFO:root:cc -pthread /tmp/tmpe_4euwzl/test.o -L/usr/local/cuda-12.6 -L/usr/local/cuda-12.6/lib64 -lcufile -o /tmp/tmpe_4euwzl/a.out\n",
      "INFO:root:cc -pthread -fno-strict-overflow -Wsign-compare -Wunreachable-code -DNDEBUG -g -O3 -Wall -fPIC -fPIC -c /tmp/tmpec_jdazh/test.c -o /tmp/tmpec_jdazh/test.o\n",
      "INFO:root:cc -pthread /tmp/tmpec_jdazh/test.o -laio -o /tmp/tmpec_jdazh/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [380/380 08:46, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.025912</td>\n",
       "      <td>1.642112</td>\n",
       "      <td>129780.000000</td>\n",
       "      <td>0.993745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>2.301833</td>\n",
       "      <td>259560.000000</td>\n",
       "      <td>0.995575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.858191</td>\n",
       "      <td>389340.000000</td>\n",
       "      <td>0.998887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>1.690483</td>\n",
       "      <td>514500.000000</td>\n",
       "      <td>0.999135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>1.709294</td>\n",
       "      <td>644280.000000</td>\n",
       "      <td>0.999380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>1.742289</td>\n",
       "      <td>774060.000000</td>\n",
       "      <td>0.999627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>1.750750</td>\n",
       "      <td>899220.000000</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>1.749091</td>\n",
       "      <td>1029000.000000</td>\n",
       "      <td>0.999630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>1.716030</td>\n",
       "      <td>1158780.000000</td>\n",
       "      <td>0.999752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>1.698164</td>\n",
       "      <td>1283940.000000</td>\n",
       "      <td>0.999752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>1.714910</td>\n",
       "      <td>1413720.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>1.727671</td>\n",
       "      <td>1543500.000000</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>1.724484</td>\n",
       "      <td>1668660.000000</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>1.721423</td>\n",
       "      <td>1798440.000000</td>\n",
       "      <td>0.999630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>1.720237</td>\n",
       "      <td>1928220.000000</td>\n",
       "      <td>0.999630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=380, training_loss=0.026182587425252056, metrics={'train_runtime': 527.4356, 'train_samples_per_second': 88.049, 'train_steps_per_second': 0.72, 'total_flos': 1191359924858880.0, 'train_loss': 0.026182587425252056, 'epoch': 20.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-timer-lora\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64bbb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gemma-timer-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d7f4859",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[32m      3\u001b[39m sample_template = \u001b[33m\"\u001b[39m\u001b[33mHOURS:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMINUTES:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSECONDS:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m max_new_tokens = \u001b[38;5;28mmax\u001b[39m(\n\u001b[32m      6\u001b[39m     [\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         \u001b[38;5;28mlen\u001b[39m(tokenizer.tokenize(sample_template.format(h, m, s)))\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m h, m, s \u001b[38;5;129;01min\u001b[39;00m product(\u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m), repeat=\u001b[32m3\u001b[39m)\n\u001b[32m      9\u001b[39m     ]\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "sample_template = \"HOURS:{}\\nMINUTES:{}\\nSECONDS:{}\"\n",
    "\n",
    "max_new_tokens = max(\n",
    "    [\n",
    "        len(tokenizer.tokenize(sample_template.format(h, m, s)))\n",
    "        for h, m, s in product(range(100), repeat=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be9f7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18c15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "840cd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f81ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./gemma-timer-lora\", device_map=\"cuda\")\n",
    "text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74656abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    dataset,\n",
    "    log_file,\n",
    "    batch_size=64,\n",
    "):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with open(log_file, \"w\") as file:\n",
    "        for i in tqdm(range(0, total, batch_size)):\n",
    "            texts = dataset[i : i + batch_size][\"text\"]\n",
    "            prefixes = [text.split(\"\\n\")[0] + \"\\n\" for text in texts]\n",
    "            gen_outs = text_gen(prefixes, max_new_tokens=max_new_tokens, num_beams=1, do_sample=False)\n",
    "            for text, gen_out in zip(texts, gen_outs):\n",
    "                gen_text = gen_out[0][\"generated_text\"]\n",
    "                if len(gen_text) >= len(text) and text == gen_text[: len(text)]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"Mismatch: {text} -> {gen_text}\\n\", file=file)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d52a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [06:10<00:00, 37.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_accuracy(dataset[\"validation\"], log_file=\"eval.log\")\n",
    "print(f\"Validation accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01e6d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: USER:start a quarter of an hour timer\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0 -> USER:start a quarter of an hour timer\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "PI\n",
      "\n",
      "Mismatch: USER:Hey assistant, set a timer for a quarter of an hour.\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0 -> USER:Hey assistant, set a timer for a quarter of an hour.\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "PI\n",
      "\n",
      "Mismatch: USER:start a timer for three minutes and forty seconds\n",
      "HOURS:0\n",
      "MINUTES:3\n",
      "SECONDS:40 -> USER:start a timer for three minutes and forty seconds\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:40\n",
      "\n",
      "\n",
      "Mismatch: USER:I need a timer set for 7 hours and 7 minutes.\n",
      "HOURS:7\n",
      "MINUTES:7\n",
      "SECONDS:0 -> USER:I need a timer set for 7 hours and 7 minutes.\n",
      "HOURS:7\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "SECONDSPI\n",
      "\n",
      "Mismatch: USER:Oh put on a 73 second timer quickly\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73 -> USER:Oh put on a 73 second timer quickly\n",
      "HOURS:0\n",
      "MINUTES:73\n",
      "SECONDS:0\n",
      "SECONDS\n",
      "\n",
      "Mismatch: USER:Start me a timer for seven hours and twelve minutes\n",
      "HOURS:7\n",
      "MINUTES:12\n",
      "SECONDS:0 -> USER:Start me a timer for seven hours and twelve minutes\n",
      "HOURS:7\n",
      "MINUTES:15\n",
      "SECONDS:0\n",
      "SECONDS\n",
      "\n",
      "Mismatch: USER:i need a timer for two hours, 20 minutes and 10 seconds\n",
      "HOURS:2\n",
      "MINUTES:20\n",
      "SECONDS:10 -> USER:i need a timer for two hours, 20 minutes and 10 seconds\n",
      "HOURS:20\n",
      "MINUTES:10\n",
      "SECONDS:10\n",
      "\n",
      "Mismatch: USER:start a timer for a quarter of an hour\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0 -> USER:start a timer for a quarter of an hour\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "PI\n",
      "\n",
      "Mismatch: USER:I want a timer for four hours and seven minutes\n",
      "HOURS:4\n",
      "MINUTES:7\n",
      "SECONDS:0 -> USER:I want a timer for four hours and seven minutes\n",
      "HOURS:4\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "SECONDSPI\n",
      "\n",
      "Mismatch: USER:I’d like a quarter hour timer\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0 -> USER:I’d like a quarter hour timer\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "AP\n",
      "\n",
      "Mismatch: USER:start a timer for 10 seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:10 -> USER:start a timer for 10 seconds\n",
      "HOURS:0\n",
      "MINUTES:10\n",
      "SECONDS:10\n",
      "\n",
      "\n",
      "Mismatch: USER:set a timer for three quarters of an hour\n",
      "HOURS:0\n",
      "MINUTES:45\n",
      "SECONDS:0 -> USER:set a timer for three quarters of an hour\n",
      "HOURS:30\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "PI\n",
      "\n",
      "Mismatch: USER:Start a timer for nine seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:9 -> USER:Start a timer for nine seconds\n",
      "HOURS:0\n",
      "MINUTES:90\n",
      "SECONDS:30\n",
      "\n",
      "\n",
      "Mismatch: USER:Timer, please! For 1 hour and a half.\n",
      "HOURS:1\n",
      "MINUTES:30\n",
      "SECONDS:0 -> USER:Timer, please! For 1 hour and a half.\n",
      "HOURS:1\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "SECONDSPI\n",
      "\n",
      "Mismatch: USER:put on a ninety-nine second timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:99 -> USER:put on a ninety-nine second timer\n",
      "HOURS:0\n",
      "MINUTES:99\n",
      "SECONDS:30\n",
      "\n",
      "\n",
      "Mismatch: USER:Please create a timer for two seconds\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:2 -> USER:Please create a timer for two seconds\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:30\n",
      "\n",
      "\n",
      "Mismatch: USER:I want a quarter of an hour timer\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0 -> USER:I want a quarter of an hour timer\n",
      "HOURS:0\n",
      "MINUTES:30\n",
      "SECONDS:0\n",
      "PI\n",
      "\n",
      "Mismatch: USER:Start a timer for a quarter of an hour.\n",
      "HOURS:0\n",
      "MINUTES:15\n",
      "SECONDS:0 -> USER:Start a timer for a quarter of an hour.\n",
      "HOURS:0\n",
      "MINUTES:45\n",
      "SECONDS:0\n",
      "PI\n",
      "\n",
      "Mismatch: USER:Set a timer for two hours twenty minutes and forty seconds\n",
      "HOURS:2\n",
      "MINUTES:20\n",
      "SECONDS:40 -> USER:Set a timer for two hours twenty minutes and forty seconds\n",
      "HOURS:2\n",
      "MINUTES:15\n",
      "SECONDS:40\n",
      "\n",
      "\n",
      "Mismatch: USER:give me a seventy-three second timer\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:73 -> USER:give me a seventy-three second timer\n",
      "HOURS:0\n",
      "MINUTES:73\n",
      "SECONDS:30\n",
      "\n",
      "\n",
      "Mismatch: USER:Please set a timer for 6 hours and half an hour.\n",
      "HOURS:6\n",
      "MINUTES:30\n",
      "SECONDS:0 -> USER:Please set a timer for 6 hours and half an hour.\n",
      "HOURS:6\n",
      "MINUTES:0\n",
      "SECONDS:0\n",
      "SECONDSPI\n",
      "\n",
      "Mismatch: USER:I'd like a timer for 80 seconds.\n",
      "HOURS:0\n",
      "MINUTES:0\n",
      "SECONDS:80 -> USER:I'd like a timer for 80 seconds.\n",
      "HOURS:0\n",
      "MINUTES:80\n",
      "SECONDS:0\n",
      "DAYS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cat eval.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir gemma-3-270m\n",
    "# !hf download google/gemma-3-270m --local-dir gemma-3-270m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d68be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmitrievan/pln/kaia_exps/gemma-timer-lora\n",
      "/home/dmitrievan/pln/kaia_exps\n"
     ]
    }
   ],
   "source": [
    "%mkdir gemma-timer-adapter-for-hf\n",
    "%cd gemma-timer-lora\n",
    "%cp adapter_model.safetensors adapter_config.json ../gemma-timer-adapter-for-hf/\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %git clone https://github.com/ggml-org/llama.cpp.git llama-cpp-repo\n",
    "# %cd llama-cpp-repo/gguf-py\n",
    "# %uv pip install .\n",
    "# %uv add mistral_common\n",
    "# %cd ../..\n",
    "# %wget https://raw.githubusercontent.com/ggml-org/llama.cpp/master/convert_lora_to_gguf.py\n",
    "# %wget https://raw.githubusercontent.com/ggml-org/llama.cpp/master/convert_hf_to_gguf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b69139b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:lora-to-gguf:Loading base model from Hugging Face: google/gemma-3-270m\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:lora-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_a,  torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_b,  torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_b,       torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_a,       torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_b,       torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_a,   torch.float32 --> F32, shape = {1024, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_b,   torch.float32 --> F32, shape = {16, 640}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_b,        torch.float32 --> F32, shape = {16, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_a,        torch.float32 --> F32, shape = {640, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_b,        torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:gemma-3-270m-lora.gguf: n_tensors = 144, total_size = 5.9M\n",
      "Writing: 100%|███████████████████████████| 5.90M/5.90M [00:00<00:00, 715Mbyte/s]\n",
      "INFO:lora-to-gguf:Model successfully exported to gemma-3-270m-lora.gguf\n"
     ]
    }
   ],
   "source": [
    "!uv run convert_lora_to_gguf.py ./gemma-timer-adapter-for-hf --outfile gemma-3-270m-lora.gguf\n",
    "# !uv run convert_hf_to_gguf.py ./gemma-3-270m --outfile gemma-3-270m.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "pln"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
